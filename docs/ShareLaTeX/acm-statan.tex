% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{SECrus}{2017, St.Petersburg, Russia}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{On development of a framework\\for massive source code analysis\\
using static code analyzers }
\subtitle{[Extended Abstract]
\titlenote{A full version of this paper is available as
\textit{Author's Guide to Preparing ACM SIG Proceedings Using
\LaTeX$2_\epsilon$\ and BibTeX} at
\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Alexander Chistyakov\\
       \affaddr{ITMO University}\\
       \affaddr{197101 Kronverkskiy, 49}\\
       \affaddr{Saint-Petersburg, Russia}\\
       \email{alexclear@gmail.com}
% 2nd. author
\alignauthor
Artem Pripadchev\\
       \affaddr{ITMO University}\\
       \affaddr{197101 Kronverkskiy, 49}\\
       \affaddr{Saint-Petersburg, Russia}\\
       \email{artem.pripadchev@outlook.com}
% 3rd. author
\alignauthor Irina Radchenko\\
       \affaddr{ITMO University}\\
       \affaddr{197101 Kronverkskiy, 49}\\
       \affaddr{Saint-Petersburg, Russia}\\
       \email{iradche@gmail.com}
\and  % use '\and' if you need 'another row' of author names
}

\maketitle
\begin{abstract}
Authors describe architecture and implementation of an automated source code
  analyzing system which uses pluggable static code analyzers. The paper
  presents a module for gathering and analyzing the source code massively in a detailed manner.
  Authors also compare existing static code analyzers for Python programming
  language. A common format of storing results of code analysis for subsequent
  processing is introduced. Also, authors discuss methods of statistical
  processing and visualizing of raw analysis data.
\end{abstract}

% A category with the (minimum) three required fields
\category{D.2.8}{Software and its engineering}[Software testing and debugging]

% \terms{Theory}

\keywords{code analysis, open source, static analyzers}

\section{Introduction}
Informational technology field is one of the fastest growing industries today.
Without doubt, using various automated systems to replace human labor especially
when doing repeatable operations is very useful. It increases effectiveness
of work and removes the possibility of accidental human-induced errors.
However, automated systems are not error-free per se. A risk of deterministic
error in program code induced not by a worker on a conveyor but by
a program creator arises \cite{item01}.

Global industrial community is very concerned about a possibility of appearance
of various types of defects in program code. To address this a number of
international standards covering software development process
were developed (ISO/IEC 90003:2014, CMM/CMMI) \cite{item02}. Moreover, a lot of
various methodologies and approaches to development of different types
of computer systems were created in past decades. Every such methodology aims
for getting a working software product in timely fashion. Some methodologies
emphasize sequence of steps of software development process, others make
development process simple and agile. Yet every such methodology targets
creating a product of good quality.

Software engineers need to define a set of measurable parameters to control the overall process
of developing a product. This can be expanded to quality of code too. It's hard
to make any project management decisions in lack of quantitative
characteristics. Thus a problem of measuring code quality is actual nowadays.

The term "quality" is quite complex and multi-dimensional. "Quality" usually
means compliance of object properties to a set of predefined requirements \cite{item03}.
Quality of program code implies thoroughly designed architecture, clear division
of code into functional submodules, defining strict structure and so on.
Software engineers use various methodologies and techniques to improve code
quality, such as using design patterns, utilizing existing libraries and
algorithms for solving typical tasks and so on \cite{item04}.

But do we have to care about code quality if end users demand another
thing - the overall quality of a product? Yes, we obviously do, because every
complex information system is a subject to evolution and modification. In this
case important metrics are number of defects in program code and a cost of
modification of code. If adding new functionality introduces a critical number
of errors the product is not able to fulfill customer needs anymore. In the
same way, if a cost of adding new functionality to the product is too high
it will affect users negatively too. Therefore, code quality is not directly
related to functionality but nonetheless important parameter which indirectly
relates to the overall program product quality.

Since code quality affects overall computer program product quality severely
we have got an idea to develop an automated code analysis system based on
existing static code analyzers.

Our idea is to use Github \cite{item16} as a provider of Python code repositories
of different size and code quality. We are going to build a system suitable for
performing massive code analysis using a predefined set of existing static code
analyzers. We chose Python mainly for two reasons: firstly, because it's fourth most popular
programming language on Github and secondly, because number of already developed
static code analyzers for Python is relatively big. Also, we can program in Python
and will be able not only fix errors in existing Python-based tools but develop
our own implementation of a static code analyzer if needed.

Our goal is to gather some raw data in result of processing code repositories from
Github using static analyzers and to normalize this raw data using a developed
common data format. Next step is to perform statistical analysis of this set of
normalized raw data using classification and clusterization and other data processing
algorithms.

\section{Related works}
Static code analysis is analysis of code conducted without real program
execution \cite{item05}. Result of such analysis in this paper is a certain analytics
which can be used to get a representation of code quality. Static code analysis
is also being used for other purposes. So, authors of \cite{item06} classify android
applications into two types: utilities and games using machine learning.
Many papers are related to finding possible vulnerabilities in programs
during development stage \cite{item07}, \cite{item08}, \cite{item09}. Authors of \cite{item10} extract code characteristics
to find defects subsequently. Another problem domain of static code analysis
is automated detection of malicious code \cite{item11}.

\section{Description of experiment}
In order to determine qualitative characteristics of program code, we should
define a set of measurable parameters first. Metrics of program code can
be used as these parameters. Essentially, this method analyzes source code
to get various numeric metrics. Usually these metrics are defined based
on analyzing either a control flow graph or a structure of program code \cite{item12}.

There are a big number of metrics representing various program code aspects
as of today. Most common metrics are number of SLOC, cyclomatic complexity,
number of warnings and errors and so on. A benefit of using metrics of program
code is absence of human factor. These metrics are measured by a computer.
This fact guarantees precision and repeatability of measurements for every
metric. Moreover, it becomes possible to measure these metrics automatedly
and to create various analytic reports based on these automated measurements.

Proposed method of measuring code quality metrics is presented in a form of
block diagram on Fig.~\ref{fig:structscheme}.

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{structscheme}
\caption{Block diagram of a framework for massive source code analysis using static code analyzers}
\label{fig:structscheme}
\end{figure}

The file preparation module is an entry point for a user. The user defines
all necessary environment for subsequent analysis. This environment consists
of a folder with project source code to be analyzed, a set of analyzers
to utilize, a set of excluded analyzer rules and so on.

After setting up the environment and starting the application system begins
to perform source code analysis. It's worth to mention that we chose Python
as a reference programming language, mainly, because Python has a lot of
scientific libraries for data processing, visualization and so on. We decided
to start off with a preexisting set of source code analyzers and then write
our own custom solution if it becomes needed. Thus, our first task was to
compare existing open source implementations of static source code
analyzing tools for Python language.

\section{Tools}

There is only a limited number of code analysis frameworks for Python with an ability
to plug different static code analyzers. Namely, they are Coala \cite{item13},
Pylama \cite{item14} and Flake8 \cite{item15}.

We defined the following set of parameters to compare these frameworks:
popularity among Github \cite{item16} users, CPU and RAM utilization, ability to
parallelize process of analysis and time required to process the same set of projects.

Since Flake8 is designed to perform style guide enforcements only and does not
have an ability to disable standard plugins easily we excluded it from further comparison.

Coala provides a uniform CLI interface for code style checking and code
improvement. Coala uses a set of plugins (called "bears") for various
programming languages. It's also possible to extend a standard set of
plugins with a custom plugin.

Pylama is a code auditing tool for Python and JavaScript programming
languages. It is not as feature rich as Coala due to lesser popularity
on Github and lower number of active contributors and code commits.

We used a virtual machine with 3Gb of RAM, 3 CPU cores and Ubuntu 14.04
installed as a test host.

\section{Results}

We chose a relatively small sample project (consisted
of roughly 130 files) and configured two plugins in both Coala and Pylama.
Processing time took 112 seconds for Coala and 183 seconds for Pylama.

But it turned out that the most important metric was RAM utilization. We created
a graphical representation of OS memory usage on Fig.~\ref{fig:memusage}.

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{memusage}
	\caption{OS memory usage when analyzing a single project by Coala and Pylama}
	\label{fig:memusage}
\end{figure}

Pylama constantly accumulates information about errors so needed RAM grows
almost linearly. This leads to a critical defect that reveals as total RAM
exhaustion. Operating system forcibly terminates Pylama then. Coala does not
accumulate errors and flushes information on errors to stdout periodically, so
the RAM does not exhaust.

Combined comparison results are presented in Table~\ref{tab:compare}. Based on
this results we decided to use Coala as a base framework for future work.
Another possible option is to patch Pylama.

\begin{table}
\centering
\caption{\label{tab:compare}Comparison of Coala and Pylama}
\begin{tabular}{|l|c|c|} \hline
			~                              & Coala & Pylama \\ \hline
			Popularity among github users  & +     & -      \\ \hline
			CPU usage                      & +    & +      \\ \hline
			RAM usage                      & +    & -      \\ \hline
			Parallel processing            & +     & -      \\ \hline
			Analysis time (sec)            & 112   & 183      \\ \hline
\hline\end{tabular}
\end{table}

Next planned step is to process results of analysis by a parsing module
and to standardize them. We are going to store standardized analysis results to
a database. We plan to evaluate a number of relational (e.g. PostgreSQL) and
non-relational databases (e.g. HBase and Cassandra).

We designed a data model and represented it as an ER diagram (Fig.~\ref{fig:dbscheme}).

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{dbscheme}
	\caption{The database schema for storing source code analysis results}
	\label{fig:dbscheme}
\end{figure}


\section{Future work}

We defined an overall structure of the system so the next step is
to build a working prototype suitable for massive code analysis. Also
we are going to define a more strict set of metrics and to start
visualizing them. Some of methods of visualizing results of measuring
quality of program products are described in \cite{item18}, \cite{item19}, \cite{item20}.

Analysis results are going to be represented as numerical metrics stored in
tables. Next step is to create a separate module to interpret these raw
standartizes results. The last step is to visualize results by a request of
end user initiated code analysis. Thus, different types of comparisons can be
done on visualization step \cite{item17}. For example, comparison can be based
on chronological characteristics of objects or we can perform per component comparison.

Resulting infographics can be presented in various forms, e.g. matrixes, maps,
figures, graphs and diagrams.

\section{Conclusion}

A problem of controlling, measuring and predicting code quality is actual
at the moment and will become even more actual in the future. Software
developers working on big projects need measurable code quality-related
metrics to improve their process. End users and other third parties need
these metrics to choose a product of best possible quality and for various
other needs. Solving this problem requires a proper instrument which scales
well and produces well-defined and predictable results. Authors of this paper
proposed an approach of creating this instrument, described its architecture
and chose a set of tools as a base to implement it.



\bibliographystyle{abbrv}
\bibliography{sigproc}  

\end{document}
